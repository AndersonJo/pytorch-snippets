{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tempfile\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import vgg19_bn\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /tmp/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5d731db3c942a3b98c565a960952b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/cifar-10-python.tar.gz to /tmp\n",
      "normalization mean: [0.49139968 0.48215841 0.44653091]\n",
      "normalization std : [0.00089181 0.00114635 0.00057204]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10 = CIFAR10(tempfile.gettempdir(),\n",
    "                  train=True,\n",
    "                  download=True)\n",
    "_mean = cifar10.data.mean(axis=0).mean(0).mean(0)/255\n",
    "_std = cifar10.data.std(axis=0).std(0).std(0)/255\n",
    "\n",
    "print('normalization mean:', _mean)\n",
    "print('normalization std :', _std)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.RandomCrop(32, padding=3),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomVerticalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(_mean, _std)])\n",
    "\n",
    "train_loader = DataLoader(CIFAR10(tempfile.gettempdir(),\n",
    "                                  train=True,\n",
    "                                  transform=transform,\n",
    "                                  download=True),\n",
    "                          batch_size=64,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(CIFAR10(tempfile.gettempdir(),\n",
    "                                 train=False,\n",
    "                                 transform=transform,\n",
    "                                 download=True),\n",
    "                         num_workers=4,\n",
    "                         batch_size=64,\n",
    "                         shuffle=True,\n",
    "                         drop_last=True, \n",
    "                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        backbone = vgg19_bn(pretrained=True)\n",
    "        backbone = nn.Sequential(* (list(list(backbone.modules())[0].children())[:-2]))\n",
    "        \n",
    "#         for param  in list(backbone.parameters())[:-25]:\n",
    "#             param.requires_grad = False\n",
    "                    \n",
    "        self.backbone = backbone\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.linear = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.backbone(x)\n",
    "        h2 = self.gap(h1)\n",
    "        b_sz, c_sz, h_sz, w_sz = h2.shape\n",
    "        h2 = h2.view(b_sz, c_sz)\n",
    "        h3 = self.linear(h2)\n",
    "        output = F.log_softmax(h3)\n",
    "        return output\n",
    "\n",
    "clf = Model()\n",
    "clf = clf.cuda()\n",
    "adam = Adam(clf.parameters(),  lr=0.01)\n",
    "lr_scheduler = ReduceLROnPlateau(adam, factor=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e033ed5228248e29f493686050f4ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=782.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:00 | loss:2.10, 1.87 | acc:0.19, 0.27 | lr:0.0100\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cf1ea256b235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-cf1ea256b235>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch_size)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mbest_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./checkpoints/checkpoint_{epoch}_{best_val_acc:.2f}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def train_epoch(clf, opt, train_loader, epoch_size:int=500):\n",
    "    clf.train()\n",
    "    n_samples = 0\n",
    "    loss_data = []\n",
    "    n_corrects = []\n",
    "    \n",
    "    prev_param = None\n",
    "    for batch_id, (data, label) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        data = data.cuda()\n",
    "        label = label.cuda()\n",
    "        \n",
    "#         if batch_id %100 == 0:\n",
    "#             param = torch.Tensor([round(p.mean().item(), 2) for p in clf.parameters()])\n",
    "#             if prev_param is not None:\n",
    "#                 diff = torch.where(prev_param != param)\n",
    "#                 if len(param[diff]):\n",
    "#                     print(diff)\n",
    "#             prev_param = param\n",
    "            \n",
    "        # Forward\n",
    "        opt.zero_grad()\n",
    "        preds = clf(data)\n",
    "        pred_probs, pred_idx = preds.max(1)\n",
    "\n",
    "        # Loss\n",
    "        loss = F.nll_loss(preds, label) # Negative Log Loss\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "        \n",
    "        # Log\n",
    "        n_samples += len(label)\n",
    "        n_corrects.append((pred_idx == label).sum().item())\n",
    "        loss_data.append(loss.item())\n",
    "        \n",
    "    \n",
    "    return {'loss': np.mean(loss_data), \n",
    "            'acc': np.sum(n_corrects)/n_samples}\n",
    "\n",
    "def test_epoch(clf, test_loader):\n",
    "    clf.eval() # set model in inference mode (need this because of dropout)\n",
    "    n_samples = 0\n",
    "    n_corrects = []\n",
    "    test_losses = []\n",
    "    \n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        output = clf(data)\n",
    "        test_loss = F.nll_loss(output, target)\n",
    "        pred_probs, pred_idx = output.max(1)\n",
    "        \n",
    "        # Log\n",
    "        n_samples += len(target)\n",
    "        n_corrects.append((pred_idx == target).sum().item())\n",
    "        test_losses.append(test_loss.item())\n",
    "    \n",
    "    return {'loss': np.mean(test_losses), \n",
    "            'acc': np.sum(n_corrects)/n_samples}\n",
    "\n",
    "        \n",
    "def train(epoch_size=1000):\n",
    "    if not os.path.exists('./checkpoints'):\n",
    "        os.makedirs('./checkpoints')\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    for epoch in range(epoch_size):\n",
    "        res_train = train_epoch(clf, adam, train_loader)  \n",
    "        res_test = test_epoch(clf, test_loader)\n",
    "        \n",
    "        lr_scheduler.step(res_test['loss'])\n",
    "        last_lr = lr_scheduler.optimizer.param_groups[0]['lr']\n",
    "        print(f'epoch:{epoch:02} | loss:{res_train[\"loss\"]:.2f}, {res_test[\"loss\"]:.2f} | '\n",
    "              f'acc:{res_train[\"acc\"]:.2f}, {res_test[\"acc\"]:.2f} | lr:{last_lr:6.4f}')\n",
    "        \n",
    "        # Save\n",
    "        if res_test['acc'] > best_val_acc:\n",
    "            best_val_acc = res_test['acc']\n",
    "            save_path = os.path.join(f'./checkpoints/checkpoint_{epoch}_{best_val_acc:.2f}.pth')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        \n",
    "train(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
